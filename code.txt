import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer
from scipy.stats import zscore
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier

# Load the data
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

# Save the 'id' column before dropping it
test_ids = test_data['id']

# Drop the 'id' column for preprocessing
train_data.drop("id", axis=1, inplace=True)
test_data.drop("id", axis=1, inplace=True)

# Handle missing values using SimpleImputer (with better strategies)
imputer = SimpleImputer(strategy='median')  # Use median for numerical columns
numerical_cols = train_data.select_dtypes(include=np.number).columns.drop("Response")

# Apply imputer on numerical columns for both train and test
train_data[numerical_cols] = imputer.fit_transform(train_data[numerical_cols])
test_data[numerical_cols] = imputer.transform(test_data[numerical_cols])

# Handle categorical missing values by filling with the most frequent value (mode)
categorical_cols = ['Gender', 'Vehicle_Age', 'Policy_Sales_Channel']
for col in categorical_cols:
    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])
    test_data[col] = test_data[col].fillna(test_data[col].mode()[0])

# Convert categorical features to 'category' type
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined dataset for all categorical columns
combined_data = pd.concat([train_data[categorical_cols], test_data[categorical_cols]], axis=0)

# Fit the encoder on the combined data and then transform both train and test data
for col in categorical_cols:
    label_encoder.fit(combined_data[col])  # Fit on combined data
    train_data[col] = label_encoder.transform(train_data[col])
    test_data[col] = label_encoder.transform(test_data[col])  # Apply the same encoding to the test data

# **Outlier Removal**: Apply Z-score method only on continuous numeric data
for col in ['Annual_Premium', 'Vintage']:
    train_data = train_data[(np.abs(zscore(train_data[col])) < 3)]  # Removes extreme outliers

# Feature Engineering: Create bins for Age and Annual_Premium and interactions
train_data['Age_Binned'] = pd.cut(train_data['Age'], bins=[0, 30, 50, 100], labels=['Young', 'Middle', 'Old'])
test_data['Age_Binned'] = pd.cut(test_data['Age'], bins=[0, 30, 50, 100], labels=['Young', 'Middle', 'Old'])

train_data['High_Premium'] = (train_data['Annual_Premium'] > train_data['Annual_Premium'].median()).astype(int)
test_data['High_Premium'] = (test_data['Annual_Premium'] > test_data['Annual_Premium'].median()).astype(int)

# Feature Interaction: Adding interaction terms, Age and Annual Premium interaction
train_data['Age_Premium_Interaction'] = train_data['Age'] * train_data['Annual_Premium']
test_data['Age_Premium_Interaction'] = test_data['Age'] * test_data['Annual_Premium']

# Encoding categorical variables using one-hot encoding
train_data = pd.get_dummies(train_data, columns=['Gender', 'Vehicle_Age', 'Age_Binned'], drop_first=True)
test_data = pd.get_dummies(test_data, columns=['Gender', 'Vehicle_Age', 'Age_Binned'], drop_first=True)

# Align columns between train and test to ensure they have the same features after one-hot encoding
X_train_full = train_data.drop("Response", axis=1)
X_test = test_data
X_train_full, X_test = X_train_full.align(X_test, join="left", axis=1, fill_value=0)

# Splitting features and target
y = train_data["Response"]

# Addressing class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train_full, y)

# --- Step 1: Feature Scaling ---
# Feature Scaling (only numerical columns)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Step 2: Cross-Validation using RandomForestClassifier ---
# Initialize the model
model = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1, max_depth=10)

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')  # 5-fold cross-validation

# Print cross-validation results
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}")
print(f"Standard Deviation of Cross-Validation Accuracy: {cv_scores.std():.4f}")

# --- Step 3: Model Training using Random Forest ---
# Fit the model on the full training data
model.fit(X_train, y_train)

# Validation
y_pred = model.predict(X_test_scaled)

# --- Step 4: Save Predictions ---
# Ensure the submission has the correct number of rows and the 'id' column is present
submission = pd.DataFrame({'id': test_ids, 'Response': y_pred})

# Save predictions
submission.to_csv("vehicle_insurance_predictions.csv", index=False)
print("Predictions saved to 'vehicle_insurance_predictions.csv'.")
